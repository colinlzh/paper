{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-66-13f572f4466c>, line 112)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-13f572f4466c>\"\u001b[0;36m, line \u001b[0;32m112\u001b[0m\n\u001b[0;31m    b=nx.DiGraph(eval(m))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*- ＃\n",
    "import sys\n",
    "import time\n",
    "import networkx as nx\n",
    "from operator import add\n",
    "def sor(l):\n",
    "    l=sorted(l,key=lambda x:x.split(\" \")[1]+x.split(\" \")[2]+x.split(\" \")[0])\n",
    "    return l\n",
    "def location_to_motif(locations):\n",
    "    dateFlag = ''\n",
    "    motifList = []\n",
    "    for i in range(0,len(locations)):\n",
    "        t=locations[i].split(\" \")\n",
    "        _date = t[1]\n",
    "        tm = t[2]\n",
    "        station = t[0]\n",
    "        if dateFlag=='' or \\\n",
    "        time.mktime(time.strptime(_date+\" \"+tm,\"%Y-%m-%d %H:%M:%S\"))-time.mktime(time.strptime(dateFlag,\"%Y-%m-%d\"))>=97200 or\\\n",
    "                (tm>\"03 \" and timeFlag<\"03\"):#if another day, initialize\n",
    "            dateFlag = _date\n",
    "            timeFlag = tm\n",
    "            motifList.append({})\n",
    "            nodeDict = {}\n",
    "        if station not in nodeDict.keys():#map station name to node num\n",
    "            nodeDict[station] = len(nodeDict)\n",
    "        if nodeDict[station] not in motifList[-1].keys():#add the appeared node to today's motif graph\n",
    "            motifList[-1][nodeDict[station]] = []\n",
    "        if (i+1)<len(locations):\n",
    "            ndate=locations[i+1].split(' ')[1]\n",
    "            ntime=locations[i+1].split(' ')[2]\n",
    "            t=time.mktime(time.strptime(_date,\"%Y-%m-%d\"))\n",
    "            tt=time.mktime(time.strptime(ndate+\" \"+ntime,\"%Y-%m-%d %H:%M:%S\"))\n",
    "            if tt-t<97200 and tm>\"03\":#yet is today's\n",
    "                n_station = locations[i+1].split(' ')[0]\n",
    "                if not n_station==station:\n",
    "                    if n_station not in nodeDict.keys():\n",
    "                        nodeDict[n_station] = len(nodeDict)\n",
    "                    if not motifList[-1][nodeDict[station]].count(nodeDict[n_station])>0:\n",
    "                        motifList[-1][nodeDict[station]].append(nodeDict[n_station])\n",
    "\n",
    "    return motifList\n",
    "def location_to_motif_anddate(locations):\n",
    "    dateFlag = ''\n",
    "    motifList = []\n",
    "    for i in range(0,len(locations)):\n",
    "        t=locations[i].split(\" \")\n",
    "        _date = t[1]\n",
    "        _time = t[2]\n",
    "        station = t[0]\n",
    "        if dateFlag=='' or \\\n",
    "         time.mktime(time.strptime(_date+\" \"+_time,\"%Y-%m-%d %H:%M:%S\"))-time.mktime(time.strptime(dateFlag,\"%Y-%m-%d\"))>=97200 or\\\n",
    "                (_time>\"03 \" and timeFlag<\"03\"):\n",
    "            dateFlag = _date #如果不进行这一步的话，说明理论上还要划归到昨天，因此后续轨迹全要划到昨天MOTIF\n",
    "            shit={}\n",
    "            timeFlag = _time\n",
    "            shit[_date]={}\n",
    "            motifList.append(shit)\n",
    "            nodeDict = {}\n",
    "        if station not in nodeDict.keys():#map station name to node num\n",
    "            nodeDict[station] = len(nodeDict)\n",
    "        if nodeDict[station] not in shit[dateFlag].keys():#add the appeared node to today's motif graph\n",
    "            shit[dateFlag][nodeDict[station]] = []\n",
    "        if (i+1)<len(locations):\n",
    "            ndate=locations[i+1].split(' ')[1]\n",
    "            ntime=locations[i+1].split(' ')[2]\n",
    "            t=time.mktime(time.strptime(_date,\"%Y-%m-%d\"))\n",
    "            tt=time.mktime(time.strptime(ndate+\" \"+ntime,\"%Y-%m-%d %H:%M:%S\"))\n",
    "            if (tt-t<97200) and _time>\"03\":#yet is today's\n",
    "                n_station = locations[i+1].split(' ')[0]\n",
    "                if not n_station==station:\n",
    "                    if n_station not in nodeDict.keys():\n",
    "                        nodeDict[n_station] = len(nodeDict)\n",
    "                    if not shit[dateFlag][nodeDict[station]].count(nodeDict[n_station])>0:\n",
    "                        shit[dateFlag][nodeDict[station]].append(nodeDict[n_station])\n",
    "    \n",
    "    return motifList.items()\n",
    "def style_conv(list_dict):\n",
    "    dicts = str(list_dict).strip('[]')\n",
    "    dicts = dicts.split(', {')\n",
    "    rsl = []\n",
    "    for i in range(0, len(dicts)):\n",
    "        if i ==0:\n",
    "            rsl.append(dicts[i])\n",
    "        else:\n",
    "            rsl.append('{' + dicts[i])\n",
    "    return rsl\n",
    "def count_motif(l):\n",
    "    dic={}\n",
    "    myset=set(l)\n",
    "    for i in myset:\n",
    "        dic[i]=l.count(i)\n",
    "    return dic\n",
    "def motifiso(l):\n",
    "# '''if there is isomorphic motif, set them in one'''\n",
    "    existed=[]\n",
    "    flag=False\n",
    "    for i in l:\n",
    "        name=i\n",
    "        lmotif=nx.DiGraph(eval(name))\n",
    "        if name in existed:\n",
    "            continue\n",
    "        for oldname in existed:\n",
    "            if len(oldname)!=len(name):\n",
    "                continue\n",
    "            oldmotif=nx.DiGraph(eval(oldname))\n",
    "            if nx.is_isomorphic(oldmotif,lmotif):\n",
    "                flag=True\n",
    "                break\n",
    "        if not flag:\n",
    "            existed.append(i)\n",
    "        flag=False\n",
    "    return existed\n",
    "def day_iso(m,l):\n",
    "    b=nx.DiGraph(eval(m))\n",
    "    for a in l:\n",
    "        if len(a)!=len(m):\n",
    "            continue\n",
    "        if nx.is_isomorphic(nx.DiGraph(eval(a)),b):\n",
    "            return a\n",
    "    return m\n",
    "def iso(m,l):\n",
    "    b=nx.DiGraph(eval(m))\n",
    "    for a in l:\n",
    "        if len(a)!=len(m):\n",
    "            continue\n",
    "        if nx.is_isomorphic(nx.DiGraph(eval(a)),b):\n",
    "            return a\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10802"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=sc.textFile(\"./part-*\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[4] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "al=data.map(lambda x:(x.split(\" \")[0],\" \".join(x.split(\" \")[1:])))\n",
    "al=al.filter(lambda x:x[1].find(\"null\")==-1 and\\\n",
    "        time.localtime(time.mktime(time.strptime(x[1].split(\" \")[1]+\" \"+x[1].split(\" \")[2],\"%Y-%m-%d %H:%M:%S\"))-10800).tm_wday<5)\n",
    "al.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "motif = al.groupByKey()\\\n",
    "            .mapValues(list)\\\n",
    "            .mapValues(sor)\\\n",
    "            .map(lambda x:(x[0],style_conv(location_to_motif(x[1]))))\n",
    "all_motif=motif.flatMap(lambda x:x[1]).distinct().collect()\n",
    "broad_motif=sc.broadcast(motifiso(all_motif))\n",
    "isomotif=motif.flatMapValues(lambda x:x)\\\n",
    "            .mapValues(lambda x:iso(x,broad_motif.value))\\\n",
    "            .groupByKey()\\\n",
    "            .mapValues(list) \n",
    "everyone_motif=isomotif.mapValues(count_motif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 146, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1914, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-64-fe9562b310cc>\", line 52, in location_to_motif_anddate\nTypeError: 'tuple' object does not support item assignment\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1914, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-64-fe9562b310cc>\", line 52, in location_to_motif_anddate\nTypeError: 'tuple' object does not support item assignment\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-e15d5ea97275>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmotifday\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msor\u001b[0m\u001b[1;33m)\u001b[0m                \u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation_to_motif_anddate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmotifday\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mD:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1310\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.3-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 146, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1914, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-64-fe9562b310cc>\", line 52, in location_to_motif_anddate\nTypeError: 'tuple' object does not support item assignment\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor79.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"D:\\ml\\spark-2.0.1-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1914, in <lambda>\n    map_values_fn = lambda kv: (kv[0], f(kv[1]))\n  File \"<ipython-input-64-fe9562b310cc>\", line 52, in location_to_motif_anddate\nTypeError: 'tuple' object does not support item assignment\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:86)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "motifday = al.groupByKey()\\\n",
    "                .mapValues(list)\\\n",
    "                .mapValues(sor)\\\n",
    "                .mapValues(location_to_motif_anddate)\n",
    "motifday.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('28054137',\n",
       " {'{0: [1, 2], 1: [2, 3], 2: [1, 0], 3: [1]}': 1,\n",
       "  '{0: [1, 2], 1: [2], 2: [3, 0], 3: [4], 4: [0]}': 1,\n",
       "  '{0: [1, 3], 1: [2, 0], 2: [1], 3: [0]}': 4,\n",
       "  '{0: [1], 1: [0]}': 9,\n",
       "  '{0: [1], 1: [2, 0], 2: [0]}': 1,\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}': 12,\n",
       "  '{0: [1], 1: [2, 0], 2: [3, 1], 3: [1]}': 1,\n",
       "  '{0: [1], 1: [2, 0], 2: [3], 3: [1]}': 8,\n",
       "  '{0: [1], 1: [2, 0], 2: [3], 3: [4, 1], 4: [3]}': 5,\n",
       "  '{0: [1], 1: [2, 0], 2: [3], 3: [4], 4: [1]}': 7,\n",
       "  '{0: [1], 1: [2, 3, 0], 2: [1], 3: [1]}': 4,\n",
       "  '{0: [1], 1: [2, 3, 0], 2: [1], 3: [4], 4: [1]}': 1,\n",
       "  '{0: [1], 1: [2, 3, 4], 2: [1], 3: [4], 4: [1, 0]}': 1,\n",
       "  '{0: [1], 1: [2, 3, 5], 2: [1], 3: [4], 4: [1], 5: [0]}': 1,\n",
       "  '{0: [1], 1: [2, 3], 2: [1], 3: [4, 0], 4: [1]}': 1,\n",
       "  '{0: [1], 1: [2, 5], 2: [3, 5], 3: [4], 4: [1], 5: [1, 0]}': 1,\n",
       "  '{0: [1], 1: [2], 2: [0]}': 11,\n",
       "  '{0: [1], 1: [2], 2: [1, 3, 0], 3: [2]}': 1,\n",
       "  '{0: [1], 1: [2], 2: [1, 3], 3: [0]}': 2,\n",
       "  '{0: [1], 1: [2], 2: [1]}': 1,\n",
       "  '{0: [1], 1: [2], 2: [3], 3: [0]}': 4,\n",
       "  '{0: [1], 1: [2], 2: [3], 3: [2, 4], 4: [0]}': 1})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "everyone_motif=isomotif.mapValues(count_motif)\n",
    "everyone_motif.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('28054137',\n",
       " ['{0: [1], 1: [2, 3, 5], 2: [1], 3: [4], 4: [1], 5: [0]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1, 3, 4], 1: [2], 2: [0], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1, 2, 3], 1: [0], 2: [0], 3: [0]}',\n",
       "  '{0: [1, 4], 1: [2], 2: [3], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 3, 4], 2: [1, 0], 3: [1], 4: [2]}',\n",
       "  '{0: [1, 4], 1: [2, 3], 2: [1], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 3], 2: [1, 0], 3: [1]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1, 2], 1: [0], 2: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1, 2], 1: [0], 2: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1, 4], 1: [2], 2: [3], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [3], 3: [0]}',\n",
       "  '{0: [1], 1: [2, 3], 2: [1], 3: [0]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1], 1: [2, 3, 0], 2: [1], 3: [1]}',\n",
       "  '{0: [1, 4], 1: [2], 2: [3], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 3], 2: [1, 0], 3: [4], 4: [2]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [3], 3: [1]}',\n",
       "  '{0: [1, 3], 1: [2, 0], 2: [1], 3: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1, 3], 1: [2, 0], 2: [1], 3: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1, 4], 1: [2, 3], 2: [1], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [1, 0]}',\n",
       "  '{0: [1, 4], 1: [2], 2: [3], 3: [0], 4: [0]}',\n",
       "  '{0: [1, 3], 1: [2, 4], 2: [0], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1], 1: [2, 3], 2: [1], 3: [4, 0], 4: [3]}',\n",
       "  '{0: [1], 1: [2], 2: [1, 3], 3: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [3, 4], 3: [2], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 5], 2: [3, 5], 3: [4], 4: [1], 5: [1, 0]}',\n",
       "  '{0: [1], 1: [2, 3, 0], 2: [1], 3: [1]}',\n",
       "  '{0: [1, 4], 1: [2, 3], 2: [1], 3: [0], 4: [0]}',\n",
       "  '{0: [1, 4], 1: [2], 2: [3], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [3], 3: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [1, 3], 3: [0]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1], 1: [2, 3], 2: [1], 3: [0]}',\n",
       "  '{0: [1, 4], 1: [2, 3], 2: [1], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [3, 1], 3: [2, 1]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1], 1: [2, 3, 0], 2: [1], 3: [2]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1, 4], 1: [2], 2: [3], 3: [0], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [3], 3: [1]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1, 2], 1: [0], 2: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [3], 3: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1, 3], 1: [2], 2: [0], 3: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1], 1: [2, 3], 2: [1], 3: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [3], 3: [2, 4], 4: [0]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [3, 1], 3: [2]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1, 2, 3], 1: [0], 2: [0], 3: [0]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1, 3], 1: [2, 0], 2: [1], 3: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [3, 0], 3: [2]}',\n",
       "  '{0: [1], 1: [2, 0], 2: [1]}',\n",
       "  '{0: [1], 1: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1], 1: [2, 3], 2: [1], 3: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [3], 3: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [0]}',\n",
       "  '{0: [1], 1: [2], 2: [1]}'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('28054137', '{0: [1], 1: [2, 3, 5], 2: [1], 3: [4], 4: [1], 5: [0]}')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isomotif.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark ",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
